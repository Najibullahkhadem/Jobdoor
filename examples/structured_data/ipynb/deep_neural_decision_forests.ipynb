{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Najibullahkhadem/Jobdoor/blob/main/examples/structured_data/ipynb/deep_neural_decision_forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEplULBzX1iu"
      },
      "source": [
        "# Classification with Neural Decision Forests\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2021/01/15<br>\n",
        "**Last modified:** 2021/01/15<br>\n",
        "**Description:** How to train differentiable decision trees for end-to-end learning in deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH-OlQ3AX1iy"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example provides an implementation of the\n",
        "[Deep Neural Decision Forest](https://ieeexplore.ieee.org/document/7410529)\n",
        "model introduced by P. Kontschieder et al. for structured data classification.\n",
        "It demonstrates how to build a stochastic and differentiable decision tree model,\n",
        "train it end-to-end, and unify decision trees with deep representation learning.\n",
        "\n",
        "## The dataset\n",
        "\n",
        "This example uses the\n",
        "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
        "provided by the\n",
        "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
        "The task is binary classification\n",
        "to predict whether a person is likely to be making over USD 50,000 a year.\n",
        "\n",
        "The dataset includes 48,842 instances with 14 input features (such as age, work class, education, occupation, and so on): 5 numerical features\n",
        "and 9 categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN9yi0tBX1i0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QI8Sj5JyX1i1"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import StringLookup\n",
        "from keras import ops\n",
        "\n",
        "\n",
        "from tensorflow import data as tf_data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjKOqlhxX1i3"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z95tWmhFX1i3",
        "outputId": "f9595059-9e24-404b-bd7b-525ca4f86a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (32561, 15)\n",
            "Test dataset shape: (16282, 15)\n"
          ]
        }
      ],
      "source": [
        "CSV_HEADER = [\n",
        "    \"age\",\n",
        "    \"workclass\",\n",
        "    \"fnlwgt\",\n",
        "    \"education\",\n",
        "    \"education_num\",\n",
        "    \"marital_status\",\n",
        "    \"occupation\",\n",
        "    \"relationship\",\n",
        "    \"race\",\n",
        "    \"gender\",\n",
        "    \"capital_gain\",\n",
        "    \"capital_loss\",\n",
        "    \"hours_per_week\",\n",
        "    \"native_country\",\n",
        "    \"income_bracket\",\n",
        "]\n",
        "\n",
        "train_data_url = (\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        ")\n",
        "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
        "\n",
        "test_data_url = (\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        ")\n",
        "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
        "\n",
        "print(f\"Train dataset shape: {train_data.shape}\")\n",
        "print(f\"Test dataset shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTCILkVdX1i4"
      },
      "source": [
        "Remove the first record (because it is not a valid data example) and a trailing\n",
        "'dot' in the class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jrm4HqTAX1i5",
        "outputId": "fd85fa0c-7eca-464c-bc5a-f9f6b6386cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-830e6b693ac9>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data.income_bracket = test_data.income_bracket.apply(\n"
          ]
        }
      ],
      "source": [
        "test_data = test_data[1:]\n",
        "test_data.income_bracket = test_data.income_bracket.apply(\n",
        "    lambda value: value.replace(\".\", \"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNCM9XFWX1i6"
      },
      "source": [
        "We store the training and test data splits locally as CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wweyxvB9X1i6"
      },
      "outputs": [],
      "source": [
        "train_data_file = \"train_data.csv\"\n",
        "test_data_file = \"test_data.csv\"\n",
        "\n",
        "train_data.to_csv(train_data_file, index=False, header=False)\n",
        "test_data.to_csv(test_data_file, index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN6mYdNbX1i7"
      },
      "source": [
        "## Define dataset metadata\n",
        "\n",
        "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
        "and encoding input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s1zmpBnkX1i7"
      },
      "outputs": [],
      "source": [
        "# A list of the numerical feature names.\n",
        "NUMERIC_FEATURE_NAMES = [\n",
        "    \"age\",\n",
        "    \"education_num\",\n",
        "    \"capital_gain\",\n",
        "    \"capital_loss\",\n",
        "    \"hours_per_week\",\n",
        "]\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
        "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
        "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
        "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
        "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
        "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
        "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
        "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
        "}\n",
        "# A list of the columns to ignore from the dataset.\n",
        "IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = \"income_bracket\"\n",
        "# A list of the labels of the target features.\n",
        "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL53puhNX1i7"
      },
      "source": [
        "## Create `tf_data.Dataset` objects for training and validation\n",
        "\n",
        "We create an input function to read and parse the file, and convert features and labels\n",
        "into a [`tf_data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
        "for training and validation. We also preprocess the input by mapping the target label\n",
        "to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PYkVHu5xX1i8"
      },
      "outputs": [],
      "source": [
        "\n",
        "target_label_lookup = StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "\n",
        "\n",
        "lookup_dict = {}\n",
        "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "    # Create a lookup to convert a string values to an integer indices.\n",
        "    # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "    lookup = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)\n",
        "    lookup_dict[feature_name] = lookup\n",
        "\n",
        "\n",
        "def encode_categorical(batch_x, batch_y):\n",
        "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
        "\n",
        "    return batch_x, batch_y\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    dataset = (\n",
        "        tf_data.experimental.make_csv_dataset(\n",
        "            csv_file_path,\n",
        "            batch_size=batch_size,\n",
        "            column_names=CSV_HEADER,\n",
        "            column_defaults=COLUMN_DEFAULTS,\n",
        "            label_name=TARGET_FEATURE_NAME,\n",
        "            num_epochs=1,\n",
        "            header=False,\n",
        "            na_value=\"?\",\n",
        "            shuffle=shuffle,\n",
        "        )\n",
        "        .map(lambda features, target: (features, target_label_lookup(target)))\n",
        "        .map(encode_categorical)\n",
        "    )\n",
        "\n",
        "    return dataset.cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr7gMqmzX1i8"
      },
      "source": [
        "## Create model inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y8xc0sAuX1i8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=\"float32\"\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=\"int32\"\n",
        "            )\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNWfkqJHX1i9"
      },
      "source": [
        "## Encode input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8DN597orX1i9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_inputs(inputs):\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            # Create a lookup to convert a string values to an integer indices.\n",
        "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
        "            value_index = inputs[feature_name]\n",
        "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
        "            )\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_feature = embedding(value_index)\n",
        "        else:\n",
        "            # Use the numerical features as-is.\n",
        "            encoded_feature = inputs[feature_name]\n",
        "            if inputs[feature_name].shape[-1] is None:\n",
        "                encoded_feature = keras.ops.expand_dims(encoded_feature, -1)\n",
        "\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    encoded_features = layers.concatenate(encoded_features)\n",
        "    return encoded_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz4fZcMkX1i9"
      },
      "source": [
        "## Deep Neural Decision Tree\n",
        "\n",
        "A neural decision tree model has two sets of weights to learn. The first set is `pi`,\n",
        "which represents the probability distribution of the classes in the tree leaves.\n",
        "The second set is the weights of the routing layer `decision_fn`, which represents the probability\n",
        "of going to each leave. The forward pass of the model works as follows:\n",
        "\n",
        "1. The model expects input `features` as a single vector encoding all the features of an instance\n",
        "in the batch. This vector can be generated from a Convolution Neural Network (CNN) applied to images\n",
        "or dense transformations applied to structured data features.\n",
        "2. The model first applies a `used_features_mask` to randomly select a subset of input features to use.\n",
        "3. Then, the model computes the probabilities (`mu`) for the input instances to reach the tree leaves\n",
        "by iteratively performing a *stochastic* routing throughout the tree levels.\n",
        "4. Finally, the probabilities of reaching the leaves are combined by the class probabilities at the\n",
        "leaves to produce the final `outputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W1HyUrnmX1i-"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralDecisionTree(keras.Model):\n",
        "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.num_leaves = 2**depth\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Create a mask for the randomly selected features.\n",
        "        num_used_features = int(num_features * used_features_rate)\n",
        "        one_hot = np.eye(num_features)\n",
        "        sampled_feature_indices = np.random.choice(\n",
        "            np.arange(num_features), num_used_features, replace=False\n",
        "        )\n",
        "        self.used_features_mask = ops.convert_to_tensor(\n",
        "            one_hot[sampled_feature_indices], dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Initialize the weights of the classes in leaves.\n",
        "        self.pi = self.add_weight(\n",
        "            initializer=\"random_normal\",\n",
        "            shape=[self.num_leaves, self.num_classes],\n",
        "            dtype=\"float32\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Initialize the stochastic routing layer.\n",
        "        self.decision_fn = layers.Dense(\n",
        "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        batch_size = ops.shape(features)[0]\n",
        "\n",
        "        # Apply the feature mask to the input features.\n",
        "        features = ops.matmul(\n",
        "            features, ops.transpose(self.used_features_mask)\n",
        "        )  # [batch_size, num_used_features]\n",
        "        # Compute the routing probabilities.\n",
        "        decisions = ops.expand_dims(\n",
        "            self.decision_fn(features), axis=2\n",
        "        )  # [batch_size, num_leaves, 1]\n",
        "        # Concatenate the routing probabilities with their complements.\n",
        "        decisions = layers.concatenate(\n",
        "            [decisions, 1 - decisions], axis=2\n",
        "        )  # [batch_size, num_leaves, 2]\n",
        "\n",
        "        mu = ops.ones([batch_size, 1, 1])\n",
        "\n",
        "        begin_idx = 1\n",
        "        end_idx = 2\n",
        "        # Traverse the tree in breadth-first order.\n",
        "        for level in range(self.depth):\n",
        "            mu = ops.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
        "            mu = ops.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
        "            level_decisions = decisions[\n",
        "                :, begin_idx:end_idx, :\n",
        "            ]  # [batch_size, 2 ** level, 2]\n",
        "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (level + 1)\n",
        "\n",
        "        mu = ops.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
        "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
        "        outputs = ops.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv54wd4AX1i-"
      },
      "source": [
        "## Deep Neural Decision Forest\n",
        "\n",
        "The neural decision forest model consists of a set of neural decision trees that are\n",
        "trained simultaneously. The output of the forest model is the average outputs of its trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iC7WcLlMX1i_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralDecisionForest(keras.Model):\n",
        "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
        "        super().__init__()\n",
        "        self.ensemble = []\n",
        "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
        "        # Each tree will have its own randomly selected input features to use.\n",
        "        for _ in range(num_trees):\n",
        "            self.ensemble.append(\n",
        "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "            )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
        "        batch_size = ops.shape(inputs)[0]\n",
        "        outputs = ops.zeros([batch_size, num_classes])\n",
        "\n",
        "        # Aggregate the outputs of trees in the ensemble.\n",
        "        for tree in self.ensemble:\n",
        "            outputs += tree(inputs)\n",
        "        # Divide the outputs by the ensemble size to get the average.\n",
        "        outputs /= len(self.ensemble)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cPGKP8FX1i_"
      },
      "source": [
        "Finally, let's set up the code that will train and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HofBTt2jX1i_"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "batch_size = 265\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    train_dataset = get_dataset_from_csv(\n",
        "        train_data_file, shuffle=True, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    model.fit(train_dataset, epochs=num_epochs)\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    print(\"Evaluating the model on the test data...\")\n",
        "    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n",
        "\n",
        "    _, accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbsIRt5SX1i_"
      },
      "source": [
        "## Experiment 1: train a decision tree model\n",
        "\n",
        "In this experiment, we train a single neural decision tree model\n",
        "where we use all input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rKC1wTD4X1jA",
        "outputId": "b4a2726b-3c31-4571-caf0-1e50b07af632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: {'age': 'age', 'education_num': 'education_num', 'capital_gain': 'capital_gain', 'capital_loss': 'capital_loss', 'hours_per_week': 'hours_per_week', 'workclass': 'workclass', 'education': 'education', 'marital_status': 'marital_status', 'occupation': 'occupation', 'relationship': 'relationship', 'race': 'race', 'gender': 'gender', 'native_country': 'native_country'}\n",
            "Received: inputs=OrderedDict([('age', 'Tensor(shape=(None,))'), ('workclass', 'Tensor(shape=(None,))'), ('fnlwgt', 'Tensor(shape=(None,))'), ('education', 'Tensor(shape=(None,))'), ('education_num', 'Tensor(shape=(None,))'), ('marital_status', 'Tensor(shape=(None,))'), ('occupation', 'Tensor(shape=(None,))'), ('relationship', 'Tensor(shape=(None,))'), ('race', 'Tensor(shape=(None,))'), ('gender', 'Tensor(shape=(None,))'), ('capital_gain', 'Tensor(shape=(None,))'), ('capital_loss', 'Tensor(shape=(None,))'), ('hours_per_week', 'Tensor(shape=(None,))'), ('native_country', 'Tensor(shape=(None,))')])\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - loss: 0.5388 - sparse_categorical_accuracy: 0.7979\n",
            "Epoch 2/10\n",
            "\u001b[1m 58/123\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3710 - sparse_categorical_accuracy: 0.8322"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3655 - sparse_categorical_accuracy: 0.8341\n",
            "Epoch 3/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3517 - sparse_categorical_accuracy: 0.8354\n",
            "Epoch 4/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3454 - sparse_categorical_accuracy: 0.8383\n",
            "Epoch 5/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.8404\n",
            "Epoch 6/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3374 - sparse_categorical_accuracy: 0.8430\n",
            "Epoch 7/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3344 - sparse_categorical_accuracy: 0.8447\n",
            "Epoch 8/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3313 - sparse_categorical_accuracy: 0.8468\n",
            "Epoch 9/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3282 - sparse_categorical_accuracy: 0.8482\n",
            "Epoch 10/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3248 - sparse_categorical_accuracy: 0.8512\n",
            "Model training finished\n",
            "Evaluating the model on the test data...\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.3459 - sparse_categorical_accuracy: 0.8330\n",
            "Test accuracy: 83.59%\n"
          ]
        }
      ],
      "source": [
        "num_trees = 10\n",
        "depth = 10\n",
        "used_features_rate = 1.0\n",
        "num_classes = len(TARGET_LABELS)\n",
        "\n",
        "\n",
        "def create_tree_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
        "\n",
        "    outputs = tree(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "tree_model = create_tree_model()\n",
        "run_experiment(tree_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmoCkTGX1jA"
      },
      "source": [
        "## Experiment 2: train a forest model\n",
        "\n",
        "In this experiment, we train a neural decision forest with `num_trees` trees\n",
        "where each tree uses randomly selected 50% of the input features. You can control the number\n",
        "of features to be used in each tree by setting the `used_features_rate` variable.\n",
        "In addition, we set the depth to 5 instead of 10 compared to the previous experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Krxpv-qBX1jA",
        "outputId": "e2c6683a-6943-425c-c325-ab3ef6dba06e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the model...\n",
            "Epoch 1/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 81ms/step - loss: 0.5502 - sparse_categorical_accuracy: 0.7931\n",
            "Epoch 2/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3639 - sparse_categorical_accuracy: 0.8366\n",
            "Epoch 3/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3466 - sparse_categorical_accuracy: 0.8381\n",
            "Epoch 4/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3402 - sparse_categorical_accuracy: 0.8395\n",
            "Epoch 5/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3359 - sparse_categorical_accuracy: 0.8405\n",
            "Epoch 6/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3295 - sparse_categorical_accuracy: 0.8433\n",
            "Epoch 8/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.3268 - sparse_categorical_accuracy: 0.8435\n",
            "Epoch 9/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.3245 - sparse_categorical_accuracy: 0.8441\n",
            "Epoch 10/10\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3228 - sparse_categorical_accuracy: 0.8455\n",
            "Model training finished\n",
            "Evaluating the model on the test data...\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 0.3791 - sparse_categorical_accuracy: 0.8231\n",
            "Test accuracy: 82.45%\n"
          ]
        }
      ],
      "source": [
        "num_trees = 25\n",
        "depth = 5\n",
        "used_features_rate = 0.5\n",
        "\n",
        "\n",
        "def create_forest_model():\n",
        "    inputs = create_model_inputs()\n",
        "    features = encode_inputs(inputs)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "    num_features = features.shape[1]\n",
        "\n",
        "    forest_model = NeuralDecisionForest(\n",
        "        num_trees, depth, num_features, used_features_rate, num_classes\n",
        "    )\n",
        "\n",
        "    outputs = forest_model(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "forest_model = create_forest_model()\n",
        "\n",
        "run_experiment(forest_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep_neural_decision_forests",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}